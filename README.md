# localCopilot
ğŸš€ Codebase Fix Assistant with RAG + LLM
Context-aware code editing using FAISS, reranking, and quantized LLM inference

ğŸ” Overview
This project is a retrieval-augmented generation (RAG) pipeline that assists developers in generating code fixes across large repositories. It combines:

ğŸ” Semantic code search using FAISS and SentenceTransformers

ğŸ§  Code-aware reranking with CrossEncoder

ğŸ¤– LLM-based fix generation using a quantized transformer model (e.g., Deepseek, Mistral, or Meta's LLaMA)

ğŸ–¥ï¸ Streamlit UI for interactive prompt construction and live editing

Built for efficient inference on consumer GPUs (RTX 3060) using 8-bit quantization.

ğŸ¯ Use Case
Give it:

A task like:
Modify classify_data function to rename variable unique_classes to different_classes

A directory of code files

And it will:

Retrieve the most relevant file(s)

Rerank results with a cross-encoder

Feed context + task into an LLM

Return and save the generated fix

ğŸ§± Architecture
mathematica
Copy
Edit
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚  Query Inputâ”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
                 â–¼                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â–¼
        â”‚  FAISS Vector DB â”‚ <â”€â”€ Code Embeddings
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ CrossEncoder Rerankerâ”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â–¼
           Context + Task
                 â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Quantized  â”‚
          â”‚  LLM (Q8)  â”‚
          â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                â–¼
          Suggested Fix
âš™ï¸ Features
âœ… File-aware retrieval (e.g., query: â€œModify testb.pyâ€¦â€ limits scope)

âœ… Automatic reranking via cross-encoder/ms-marco-MiniLM-L-6-v2

âœ… Support for 8-bit quantized models using BitsAndBytesConfig

âœ… On-device inference (tested on RTX 3060, 6GB VRAM)

âœ… Token-aware trimming and output parsing

âœ… Streamlit UI with live input, output, and cache clearing

ğŸ–¥ï¸ UI Snapshot

ğŸ§ª Example Prompt
Input:

Modify testb.py so it uses Metaâ€™s LLaMA LLM instead of Deepseek

Generated Fix:

python
Copy
Edit
model_name = "meta-llama/Llama-2-7b-chat-hf"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto")
ğŸ“¦ Setup Instructions
ğŸ§° Requirements
Python 3.9+

CUDA-enabled GPU (recommended: RTX 3060+)

Packages:

bash
Copy
Edit
pip install torch torchvision
pip install sentence-transformers faiss-cpu transformers accelerate bitsandbytes
pip install streamlit scikit-learn

â–¶ï¸ Run the App
bash
Copy
Edit
streamlit run try.py

ğŸ§  Models Used
Component	Model Name
Embedding Model	all-MiniLM-L6-v2 (SentenceTransformer)
Reranker	cross-encoder/ms-marco-MiniLM-L-6-v2
LLM	deepseek-ai/deepseek-coder-1.3b-instruct (or LLaMA/Mistral/etc)


ğŸ§¾ License
MIT License â€” feel free to fork, modify, and extend.
